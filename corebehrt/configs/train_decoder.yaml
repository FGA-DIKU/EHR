logging:
  level: INFO
  path: ./outputs/logs

paths:
## INPUTS
  prepared_data: ./outputs/pretraining/processed_data/
  # restart_model: ... # Use for restarting from checkpoint

## OUTPUTS
  model: ./outputs/decoder # Save model/outputs to this folder
  #runs: ./outputs/pretraining # Use for generating a new model folder
  
trainer_args:
  batch_size: 32
  effective_batch_size: 64
  epochs: 1
  info: true
  sampler: null
  gradient_clip: 
    clip_value: 1.0
  shuffle: true
  early_stopping: null # num_epochs or null/false

model:
  n_embd: 768
  n_layer: 12
  n_head: 12
  n_inner: 48
  type_vocab_size: 1024 # !!! type_vocab_size should be > truncation_len//2 if sep token else >truncation len
  n_positions: 1024 # the longest sequence length we will use this model with
  embedding_dropout: 0.1
 
optimizer:
  lr: 5e-4
  eps: 1e-6

scheduler:
  _target_: transformers.get_linear_schedule_with_warmup
  num_warmup_epochs: 2
  num_training_epochs: 3

metrics:
  perplexity:
    _target_: corebehrt.modules.monitoring.metrics.Perplexity
  ce_loss:
    _target_: corebehrt.modules.monitoring.metrics.LossAccessor
    loss_name: loss
  next_token_precision_at_10:
    _target_: corebehrt.modules.monitoring.metrics.NextTokenPrecisionAtK
    topk: 10