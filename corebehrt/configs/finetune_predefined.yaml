logging:
  level: INFO
  path: ./outputs/logs

paths:
## INPUTS
  features: ./outputs/features
  tokenized: ./outputs/tokenized
  cohort: ./outputs/cohort # path to cohort directory
  # test_pids: pids_path # path to file with pids to use for test set

  # tokenized_file: "features_finetune.pt" # can also be a list
  # tokenized_pids: "pids_finetune.pt" # can also be a list
 
  pretrain_model: ./outputs/pretraining
 
  # restart_model: ... # Use for restarting from checkpoint
  
  outcome: ./outputs/outcomes/TEST_OUTCOME.csv
  
  

## OUTPUTS
  model: ./outputs/finetuning # Save model/outputs to this folder
  #runs: ./outputs/pretraining # Use for generating a new model folder
  
model:
  cls:
    _target_: ehr2vec.model.heads.ClassifierGRU
    bidirectional: true

data:
  cv_folds: 2 # fo splitting data into k folds
  test_split: 0.2 # only used if predefined_folds is false and test_pids is not provided
  val_split: 0.2 # only used if predefined_folds is false and cv_folds is 1
  predefined_folds: true # using predefined folds, ignore cv_folds if true
  truncation_len: 30
  min_len: 2 # 0 by default


outcome: # we will convert outcomes to binary based on whether at least one outcome is in the follow up window
  n_hours_censoring: -10 # censor time after index date (negative means before)
  n_hours_start_follow_up: 1 # start follow up (considering outcomes) time after index date (negative means before)
  n_hours_end_follow_up: null # end follow up (considering outcomes) time after index date (negative means before)
  

trainer_args:
  sampler: true
  sample_weight_function:
    _target_: corebehrt.evaluation.utils.inverse_sqrt # function to calculate sample weights
  batch_size: 8
  val_batch_size: 16
  effective_batch_size: 16
  epochs: 3
  info: true
  gradient_clip: 
    clip_value: 1.0
  shuffle: true
  checkpoint_frequency: 1
  early_stopping: 20
  stopping_criterion: roc_auc
  
optimizer:
  lr: 5e-4
  eps: 1e-6

scheduler:
  _target_: transformers.get_linear_schedule_with_warmup
  num_warmup_steps: 10
  num_training_steps: 100

metrics:
  accuracy:
    _target_: corebehrt.modules.monitoring.metrics.Accuracy
    threshold: 0.6
  roc_auc:
    _target_: corebehrt.modules.monitoring.metrics.ROC_AUC

  pr_auc:
    _target_: corebehrt.modules.monitoring.metrics.PR_AUC
  precentage_positives:
    _target_: corebehrt.modules.monitoring.metrics.Percentage_Positives

  mean_probability:
    _target_: corebehrt.modules.monitoring.metrics.Mean_Probability
  true_positives:
    _target_: corebehrt.modules.monitoring.metrics.True_Positives

  true_negatives:
    _target_: corebehrt.modules.monitoring.metrics.True_Negatives
  false_positives:
    _target_: corebehrt.modules.monitoring.metrics.False_Positives

  false_negatives:
    _target_: corebehrt.modules.monitoring.metrics.False_Negatives
  
  

 
