logging:
  level: INFO           # Defines the logging level (INFO, DEBUG, ERROR, etc.)
  path: ./outputs/logs   # Specifies the directory where log files will be stored

paths:
## INPUTS
  prepared_data: ./outputs/finetuning/processed_data/    # Path to preprocessed data for fine-tuning
  test_pids: ./outputs/cohort_absolute/test_pids.pt   # Path to file containing patient IDs for test set (OOT)
  pretrain_model: ./outputs/pretraining      # Directory containing pre-trained model
  cohort: ./outputs/cohort # Directory where the selected cohort is saved

  # restart_model: ... # Optional: use this block to resume training from a checkpoint

## OUTPUTS
  model: ./outputs/finetuning    # Directory to save fine-tuned model and outputs
  #runs: ./outputs/pretraining   # _Optional_ directory to generate a new model folder

evaluate: true     # Whether to evaluate best model on test set after training

model:
  cls:
    _target_: ehr2vec.model.heads.ClassifierGRU     # GRU-based classifier head
    bidirectional: true                             # Enable bidirectional GRU to capture past and future context

data:
  test_split: 0.3 # Use test_fraction to split data only if predefined_folds is False and test_pids are not provided
  val_split: 0.3  # Fraction of remaining data for validation (only used when cv_folds is 1)
  predefined_folds: true # If true, use provided patient ID files instead of random splits
  truncation_len: 64  # Maximum sequence length (truncate longer sequences)
  min_len: 2 #  # Minimum sequence length required for a sample to be included _0 by default


outcome: # we will convert outcomes to binary based on whether at least one outcome is in the follow up window
  n_hours_censoring: -10 # Hours before - after index time to censor events (negative = before index)
  n_hours_start_follow_up: 1  # Hours after index to start considering outcomes (negative = before index)
  n_hours_end_follow_up: null # end follow up (considering outcomes) time after index date (negative means before)  

trainer_args:
  sampler: true
  sample_weight_function:
    _target_: corebehrt.evaluation.utils.inverse_sqrt # function to calculate sample weights
  batch_size: 128
  val_batch_size: 128
  effective_batch_size: 128
  epochs: 1
  info: true
  gradient_clip: 
    clip_value: 1.0
  shuffle: true
  checkpoint_frequency: 1
  early_stopping: 20
  stopping_criterion: roc_auc
  
optimizer:
  lr: 5e-4
  eps: 1e-6

scheduler:
  _target_: transformers.get_linear_schedule_with_warmup
  num_warmup_steps: 10
  num_training_steps: 100

metrics:
  accuracy:
    _target_: corebehrt.modules.monitoring.metrics.Accuracy
    threshold: 0.6
  roc_auc:
    _target_: corebehrt.modules.monitoring.metrics.ROC_AUC

  pr_auc:
    _target_: corebehrt.modules.monitoring.metrics.PR_AUC
  precentage_positives:
    _target_: corebehrt.modules.monitoring.metrics.Percentage_Positives

  

 
