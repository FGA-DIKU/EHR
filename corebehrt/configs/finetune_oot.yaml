logging:
  level: INFO           # Defines the logging level (INFO, DEBUG, ERROR, etc.)
  path: ./outputs/logs   # Specifies the directory where log files will be stored

paths:
## INPUTS
  prepared_data: ./outputs/finetuning/processed_data/    # Path to preprocessed data for fine-tuning
  test_pids: ./outputs/cohort_absolute/test_pids.pt   # Path to file containing patient IDs for test set (OOT)
  pretrain_model: ./outputs/pretraining      # Directory containing pre-trained model
  cohort: ./outputs/cohort # Directory where the selected cohort is saved

  # restart_model: ... # Optional: use this block to resume training from a checkpoint

## OUTPUTS
  model: ./outputs/finetuning    # Directory to save fine-tuned model and outputs
  #runs: ./outputs/pretraining   # _Optional_ directory to generate a new model folder

evaluate: true     # Whether to evaluate best model on test set after training

model:
  cls:
    _target_: ehr2vec.model.heads.ClassifierGRU     # GRU-based classifier head
    bidirectional: true                             # Enable bidirectional GRU to capture past and future context

data:
  test_split: 0.3 # Use test_fraction to split data only if predefined_folds is False and test_pids are not provided
  val_split: 0.3  # Fraction of remaining data for validation (only used when cv_folds is 1)
  predefined_folds: true # If true, use provided patient ID files instead of random splits
  truncation_len: 64  # Maximum sequence length (truncate longer sequences)
  min_len: 2 #  # Minimum sequence length required for a sample to be included _0 by default


outcome: # we will convert outcomes to binary based on whether at least one outcome is in the follow up window
  n_hours_censoring: -10 # Hours before - after index time to censor events (negative = before index)
  n_hours_start_follow_up: 1  # Hours after index to start considering outcomes (negative = before index)
  n_hours_end_follow_up: null # end follow up (considering outcomes) time after index date (negative means before)  

trainer_args:
  sampler: true         # Whether to use a sampler for balancing classes or weighting
  sample_weight_function:
    _target_: corebehrt.evaluation.utils.inverse_sqrt    # Function to compute sample weights (inverse square root of frequency)
  batch_size: 128         # Batch size for training
  val_batch_size: 128     # Batch size for validation
  effective_batch_size: 128   # Batch size after gradient accumulation (used for memory efficiency)
  epochs: 1       # Number of training epochs
  info: true      #Print training progress/info
  gradient_clip:    
    clip_value: 1.0   # Clip gradients to prevent exploding values
  shuffle: true     # Shuffle training data before each epoch
  checkpoint_frequency: 1   # Save model checkpoint every N epochs
  early_stopping: 20        # Stop training if no improvement after N epochs
  stopping_criterion: roc_auc   # Metric to monitor for early stopping
  
optimizer:
  lr: 5e-4 # Learning rate for the optimizer
  eps: 1e-6  # Small epsilon value to prevent division by zero in Adam optimizer

scheduler:
  _target_: transformers.get_linear_schedule_with_warmup
  num_warmup_steps: 10 # Number of warmup steps (gradually increases learning rate at the beginning)
  num_training_steps: 100  #Total number of training steps


metrics:     # List of evaluation metrics for model performance monitoring
  accuracy:
    _target_: corebehrt.modules.monitoring.metrics.Accuracy 
    threshold: 0.6    # Classification threshold for accuracy
  roc_auc:
    _target_: corebehrt.modules.monitoring.metrics.ROC_AUC   # Area under the Receiver Operating Characteristic curve

  pr_auc:
    _target_: corebehrt.modules.monitoring.metrics.PR_AUC    # Area under the Precision-Recall curve
  precentage_positives:
    _target_: corebehrt.modules.monitoring.metrics.Percentage_Positives   # Percentage of positive predictions
