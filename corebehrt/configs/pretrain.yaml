logging:
  level: INFO  # Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
  path: ./outputs/logs

# Paths for input/output files
paths:
## INPUTS
  prepared_data: ./outputs/pretraining/processed_data/
  # restart_model: ... # Use for restarting from checkpoint

## OUTPUTS
  model: ./outputs/pretraining  # Path to save trained model
  #runs: ./outputs/pretraining  # Use for generating a new model folder
  
data:
  dataset:
    select_ratio: 1. # Fraction of dataset to use (1.0 means 100%)
    masking_ratio: .8 # Ratio of tokens to mask for masked language modeling
    replace_ratio: .1 # Ratio of masked tokens replaced with random tokens
    ignore_special_tokens: true # Ignore special tokens when processing data
  truncation_len: 20 # Maximum sequence length after truncation
  min_len: 2 # Minimum sequence length required for a valid sample
  cutoff_date:
    year: 2020
    month: 1
    day: 1

# Training arguments
trainer_args:
  batch_size: 32       # Number of samples per batch
  effective_batch_size: 64  # Total batch size after gradient accumulation
  epochs: 5  # Number of training epochs
  info: true  # Whether to print training info
  sampler: null  # Custom sampler for training (set to null for default)
  gradient_clip: 
    clip_value: 1.0
  shuffle: true
  early_stopping: null # num_epochs or null/false

model:
  hidden_size: 96
  num_hidden_layers: 3
  num_attention_heads: 3
  intermediate_size: 64
  
  type_vocab_size: 1024 # !!! type_vocab_size should be > truncation_len//2 if sep token else >truncation len
  embedding_dropout: 0.1 
  max_position_embeddings: 512 # the longest sequence length we will use this model with
  
optimizer:
  lr: 5e-4
  eps: 1e-6

scheduler:
  _target_: transformers.get_linear_schedule_with_warmup
  num_warmup_epochs: 2
  num_training_epochs: 3

metrics:
  top1:
    _target_: corebehrt.modules.monitoring.metrics.PrecisionAtK
    topk: 1
  top10:
    _target_: corebehrt.modules.monitoring.metrics.PrecisionAtK

    topk: 10
  mlm_loss:
    _target_: corebehrt.modules.monitoring.metrics.LossAccessor
    loss_name: loss
