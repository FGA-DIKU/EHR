@article{odgaard2024core,
  title={Core-behrt: A carefully optimized and rigorously evaluated behrt},
  author={Odgaard, Mikkel and Klein, Kiril Vadimovic and Thysen, Sanne M{\o}ller and Jimenez-Solem, Espen and Sillesen, Martin and Nielsen, Mads},
  journal={arXiv preprint arXiv:2404.15201},
  year={2024},
  doi={10.48550/arXiv.2404.15201}
}

@article{kolo2024meds,
  title={MEDS Decentralized, Extensible Validation (MEDS-DEV) Benchmark: Establishing Reproducibility and Comparability in ML for Health},
  author={Kolo, Aleksia and Pang, Chao and Choi, Edward and Steinberg, Ethan and Jeong, Hyewon and Gallifant, Jack and Fries, Jason A and Chiang, Jeffrey N and Oh, Jungwoo and Xu, Justin and others},
  year={2024},
  url={https://openreview.net/pdf?id=DExp3tRRel},
  note = {Accessed: 2025-10-08}
}

@article{li2020behrt,
  title={BEHRT: transformer for electronic health records},
  author={Li, Yikuan and Rao, Shishir and Solares, Jos{\'e} Roberto Ayala and Hassaine, Abdelaali and Ramakrishnan, Rema and Canoy, Dexter and Zhu, Yajie and Rahimi, Kazem and Salimi-Khorshidi, Gholamreza},
  journal={Scientific reports},
  volume={10},
  number={1},
  pages={7155},
  year={2020},
  publisher={Nature Publishing Group UK London},
  doi={10.1038/s41598-020-62922-y}
}

@article{warner2024smarter,
  title={Smarter, better, faster, longer: A modern bidirectional encoder for fast, memory efficient, and long context finetuning and inference},
  author={Warner, Benjamin and Chaffin, Antoine and Clavi{\'e}, Benjamin and Weller, Orion and Hallstr{\"o}m, Oskar and Taghadouini, Said and Gallagher, Alexis and Biswas, Raja and Ladhak, Faisal and Aarsen, Tom and others},
  journal={arXiv preprint arXiv:2412.13663},
  year={2024},
  doi={10.48550/arXiv.2412.13663}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023},
  doi={10.48550/arXiv.2302.13971}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020},
  doi={10.48550/arXiv.2005.14165}
}

@inproceedings{devlin2019bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers)},
  pages={4171--4186},
  year={2019},
  doi={10.48550/arXiv.1810.04805}
}

@article{pang2024cehr,
  title={CEHR-GPT: Generating electronic health records with chronological patient timelines},
  author={Pang, Chao and Jiang, Xinzhuo and Pavinkurve, Nishanth Parameshwar and Kalluri, Krishna S and Minto, Elise L and Patterson, Jason and Zhang, Linying and Hripcsak, George and G{\"u}rsoy, Gamze and Elhadad, No{\'e}mie and others},
  journal={arXiv preprint arXiv:2402.04400},
  year={2024},
  doi={10.48550/arXiv.2402.04400}
}

@inproceedings{pang2021cehr,
  title={CEHR-BERT: Incorporating temporal information from structured EHR data to improve prediction tasks},
  author={Pang, Chao and Jiang, Xinzhuo and Kalluri, Krishna S and Spotnitz, Matthew and Chen, RuiJun and Perotte, Adler and Natarajan, Karthik},
  booktitle={Machine Learning for Health},
  pages={239--260},
  year={2021},
  organization={PMLR},
  doi={10.48550/arXiv.2111.08585}
}

@article{gu2023mamba,
  title={Mamba: Linear-time sequence modeling with selective state spaces},
  author={Gu, Albert and Dao, Tri},
  journal={arXiv preprint arXiv:2312.00752},
  year={2023},
  doi={10.48550/arXiv.2312.00752}
}

@article{rasmy2021med,
  title={Med-BERT: pretrained contextualized embeddings on large-scale structured electronic health records for disease prediction},
  author={Rasmy, Laila and Xiang, Yang and Xie, Ziqian and Tao, Cui and Zhi, Degui},
  journal={NPJ digital medicine},
  volume={4},
  number={1},
  pages={86},
  year={2021},
  publisher={Nature Publishing Group UK London},
  doi={10.1038/s41746-021-00455-y}
}

@incollection{pytorch_2019,
  title     = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  author    = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  booktitle = {Advances in Neural Information Processing Systems 32},
  pages     = {8024--8035},
  year      = {2019},
  publisher = {Curran Associates, Inc.},
  url       = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf},
  doi={10.48550/arXiv.1912.01703}
}

@misc{huggingface_transformers,
  title = {Transformers},
  howpublished = {\url{https://huggingface.co/docs/transformers/en/index}},
  note = {Accessed: 2025-09-25}
}

@misc{mlflow,
  title = {mlflow.azureml},
  howpublished = {\url{https://mlflow.org/docs/1.22.0/python_api/mlflow.azureml.html}},
  note = {Accessed: 2025-09-25}
}